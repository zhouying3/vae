#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Oct 25 19:02:15 2017

@author: zhouying
"""

import tensorflow as tf
import numpy as np
from myutil import Dataset
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
data = min_max_scaler.fit_transform(X)
min_max_scaler = preprocessing.MinMaxScaler()
label = min_max_scaler.fit_transform(Y)

#mnist = Dataset(X,Y)

learning_rate = 0.0001
batchsize = 10
total_step = 50
input_dim = 6
hidden_encoder_dim = 4
output_dim = 2

x = tf.placeholder("float", shape=[None, input_dim])
y = tf.placeholder("float", shape=[None, output_dim])

def weight_variable(shape):
        initial = tf.truncated_normal(shape, stddev=0.001)
        return tf.Variable(initial)
def bias_variable(shape):
        initial = tf.constant(0., shape=shape)
        return tf.Variable(initial)
    
W_1 = weight_variable([input_dim,hidden_encoder_dim])
b_1 = bias_variable([hidden_encoder_dim])

W_2 = weight_variable([hidden_encoder_dim,output_dim])
b_2 = bias_variable([output_dim])

z = tf.nn.sigmoid(tf.matmul(x,W_1)+b_1)
output = tf.matmul(z,W_2)+b_2

loss = y-output

sess = tf.Session()
train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)
sess.run(tf.global_variables_initializer())
for i in range(total_step):
#    batch = mnist.next_batch(batchsize)
#    print(batch.shape)
    _, cur_loss = sess.run([train_step,loss], feed_dict={x:data,y:label})
#    print(cur_loss)
    print("Error:" + str(np.mean(np.abs(cur_loss))))